# Objective

Develop a Python tool to process `.dat` files and ZIP archives containing `.dat` files, transforming them into specified
output formats (JSONL, Parquet, Ion, and CSV) while validating against predefined schemas (AM and EN).

# Ground Rules

When reading any EBNF grammar in this document:

- Each rule must be expanded mechanically, one at a time.

When writing code

- Separate each file so it is easy to copy/paste
- Include import statements

# Part 1: Generate Pydantic Data Objects

## AM Record Schema

* Use the table immediately following this for the AM Record Schema
* If the table does not exist, extract the schema from the provided `Public Access Database Definitions`

| Position | Data Element                | Definition                        | Data Type    |
|----------|-----------------------------|-----------------------------------|--------------|
| 1        | Record Type [AM]            | Record type                       | char(2)      |
| 2        | Unique System Identifier    | Unique identifier for the system  | numeric(9,0) |
| 3        | ULS File Number             | ULS file number                   | char(14)     |
| 4        | EBF Number                  | EBF number                        | varchar(30)  |
| 5        | Call Sign                   | Call sign                         | char(10)     |
| 6        | Operator Class              | Operator class                    | char(1)      |
| 7        | Group Code                  | Group code                        | char(1)      |
| 8        | Region Code                 | Region code                       | tinyint      |
| 9        | Trustee Call Sign           | Trustee call sign                 | char(10)     |
| 10       | Trustee Indicator           | Trustee indicator                 | char(1)      |
| 11       | Physician Certification     | Physician certification           | char(1)      |
| 12       | VE Signature                | Volunteer Examiner (VE) signature | char(1)      |
| 13       | Systematic Call Sign Change | Systematic call sign change       | char(1)      |
| 14       | Vanity Call Sign Change     | Vanity call sign change           | char(1)      |
| 15       | Vanity Relationship         | Vanity relationship               | char(12)     |
| 16       | Previous Call Sign          | Previous call sign                | char(10)     |
| 17       | Previous Operator Class     | Previous operator class           | char(1)      |
| 18       | Trustee Name                | Name of trustee                   | varchar(50)  |

## EN Record Schema

* Use the table immediately following this for the EN Record Schema
* If the table does not exist, extract the schema from the provided `Public Access Database Definitions`

| Position | Data Element                    | Definition                       | Data Type    |
|----------|---------------------------------|----------------------------------|--------------|
| 1        | Record Type [EN]                | Record type                      | char(2)      |
| 2        | Unique System Identifier        | Unique identifier for the system | numeric(9,0) |
| 3        | ULS File Number                 | ULS file number                  | char(14)     |
| 4        | EBF Number                      | EBF number                       | varchar(30)  |
| 5        | Call Sign                       | Call sign                        | char(10)     |
| 6        | Entity Type                     | Type of entity                   | char(2)      |
| 7        | Licensee ID                     | Licensee identifier              | char(9)      |
| 8        | Entity Name                     | Name of the entity               | varchar(200) |
| 9        | First Name                      | First name of individual         | varchar(20)  |
| 10       | MI                              | Middle initial                   | char(1)      |
| 11       | Last Name                       | Last name                        | varchar(20)  |
| 12       | Suffix                          | Name suffix                      | char(3)      |
| 13       | Phone                           | Phone number                     | char(10)     |
| 14       | Fax                             | Fax number                       | char(10)     |
| 15       | Email                           | Email address                    | varchar(50)  |
| 16       | Street Address                  | Street address                   | varchar(60)  |
| 17       | City                            | City                             | varchar(20)  |
| 18       | State                           | State                            | char(2)      |
| 19       | Zip Code                        | Zip code                         | char(9)      |
| 20       | PO Box                          | PO Box                           | varchar(20)  |
| 21       | Attention Line                  | Attention line                   | varchar(35)  |
| 22       | SGIN                            | Signature identifier             | char(3)      |
| 23       | FCC Registration Number (FRN)   | FCC registration number          | char(10)     |
| 24       | Applicant Type Code             | Applicant type code              | char(1)      |
| 25       | Applicant Type Code Other       | Other applicant type code        | char(40)     |
| 26       | Status Code                     | Status code                      | char(1)      |
| 27       | Status Date                     | Status date                      | mm/dd/yyyy   |
| 28       | 3.7 GHz License Type            | 3.7 GHz license type             | char(1)      |
| 29       | Linked Unique System Identifier | Linked unique system identifier  | numeric(9,0) |
| 30       | Linked Call Sign                | Linked call sign                 | char(10)     |

## Tasks

### Task 1. Generate Pydantic data object for provided schemas 

Place the schema record types in src/schema/schemas.py

### Task 2. 

# Part 1: Build the .dat file Finite State Machine

## .dat file format

```text
untyped-record = ( untyped-field, "|" )+, eol ;
untyped-field = untyped-field-byte+ ;
untyped-field-byte = ? any character except "|" ? ;

eol = nl | cr, nl ;
nl = "\n" ;
cr = "\r" ;
```

## Constraints

### Length Limit Constraints

- **untyped-field**:
    - Minimum length: 0 bytes (empty field).
    - Maximum length: 1024 bytes.

### Size Limit Constraints

- **untyped-record**:
    - Minimum number of untyped-fields: 0 (empty record).
    - Maximum number of untyped-fields: 256.

### Additional Constraints

It is invalid when a stream ends in a partial record.

## Key Considerations

- A `.dat` file can be empty (no records).
- An **untyped-record** can have no fields.
- an untyped-record will always end with a pipe(|) immediately followed by a NL or a CR NL.

## Valid Samples

1. Empty untyped-fields

```text
|||||
```

2. Basic records with multiple fields

```text
field1|field2|field3|field4|\n
```

3. Multiple records

```text
a|b|c|d|
1|2|3|4|
```

4. single record where `field-value`s have newlines

```text
\nab|c\nd|ef\n|
```

## Invalid Samples

1. Unterminated record

```text
field1
```

## Tasks

### Task 1: Generate a "Sequential Component Analysis" from the Grammar

- Create a detailed analysis of the components in the grammar and their relationships.
- Ensure the output can be copied and pasted into a monospace text document for clarity.

### Task 2: Generate a "Rule to State Mapping" from the Grammar

- Map each grammar rule to a corresponding state in the finite state machine.
- Include transitions and substate details where applicable.
- Ensure the output is suitable for use in a monospace text document.

### Task 3: Generate a "State Transition Table" from the Grammar

- Define a table representing all possible transitions between states based on input.
- Include constraints and edge cases in the table.
- Ensure the output format is monospace text document-friendly.
- Add additional states to handle CR
    - Use the _CR suffix for states where you are waiting for a NL after a CR

# Part 2: Build a Finite State Machine (FSM)

Build a FSM to process a stream of .dat files

* Start with the state machine in the `State Transition Table`
* Add states for multiple records
* Implement using the State pattern

## Validation, Error Handling, and Reporting

Enforce grammar and other constraints (e.g, length, size)

When a constraint check fails

* Set the FSM in an unrecoverable and halted ERROR state.
* raise an Error with debugging context, including
    * The stream line number
    * What the error was. For example, an invalid character
    * What was expected (like untyped-field-char) and what was received. Use ascii if the value is whitespace or
      unprintable
* Further input into the FSM will result in the same raised Error

## Tasks

### Task 1: Build the Context FSM classes

Put them in dat/fsm/context.py

### Task 2: Build the State FSM classes

Put them in dat/fsm/state.py

# Part 3: Build the Parsers

## UntypedRecord

UntypedRecord class

* has a list of strings that represent a untyped-field

## Common Requirements

* Parsers will convert a .dat stream into `UntypedRecord`s
* Use the contex class to drive the state machine

## Tasks

### Task 1: Build the UntypedRecord

Put it in dat/untyped_record.py

### Task 2: Build a DatPullParser

* Accepts a stream
* methods are next and hasNext

Put it in dat/parser/pull_parser.py


